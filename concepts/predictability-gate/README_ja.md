# predictability-gate

**AIに「判断させない」ための運用上の境界定義**

このリポジトリは、  
AIモデルを作り始める**前**に、

> **その判断を AI に委ねてよいかどうか**

を決めるための、  
**運用上の Gate（関門）**を記録したものです。

これはプロダクトではありません。  
フレームワークでもありません。  
ベストプラクティスでもありません。

**責任・安全・予測可能性が保証できない場合に、  
AI の判断を止めるための運用構造**です。

---

## これは何か（What this is）

`predictability-gate` は、実務の中で使われている  
**事前判定用の意思決定ゲート**です。

問いは一つだけです。

> **この判断を、AI に委ねてよいか？**

この Gate は、次のような状況を防ぐために存在します。

- 技術的にはモデルが作れる  
- 精度もそれなりに出ている  
- しかし **一度の誤判定が、取り返しのつかない結果を招く**

本リポジトリは  
**モデルを良くするための資料ではなく、  
判断を止めるための構造**を扱います。

---

## これは何ではないか（What this is NOT）

- ❌ 機械学習フレームワーク  
- ❌ AIガバナンス標準  
- ❌ コンプライアンスチェックリスト  
- ❌ 安全性を保証するもの  

このリポジトリは  
**万能性や完全性を主張しません**。

あくまで、  
**現実の責任を背負う現場で使われた一つの構造例**です。

---

## 基本原則（Core principles）

Predictability Gate は、次の原則を前提とします。

1. **精度よりも責任を優先する**  
2. **「当たるか」より「外したらどうなるか」を問う**  
3. **不可逆な見逃しは、AIに委ねてはならない**  
4. **最終判断は、必ず人が引き受ける**

これらが満たせない場合、  
**正しい結論は「使わない」ことです。**

---

## 適用範囲（Scope）

本 Gate は、主に  
**数値時系列データを扱う AI 案件**を対象としています。

以下を主要判断軸とする案件は、  
意図的に **別プロジェクト** として分離します。

- 音  
- におい  
- 画像  
- 人の感覚・勘・経験  

これらを早期に混在させないことは、  
安全と成功確率のための**意図的な設計**です。

本リポジトリでは、Gate 定義の前に
**なぜこの境界が必要になったのか（失敗パターン）** を明示します。

---

## Gate の構成（Structure）

Predictability Gate は、  
**48〜72時間以内**に以下の観点で評価されます。

### Gate 項目

- **G1：情報量**  
  数値時系列に、予測に足る情報が含まれているか

- **G2：業務許容下限**  
  誤判定時に  
  「安全側に倒れる」「判断保留できる」逃げ道があるか

- **G3：時序整合**  
  時系列 CV やシャッフル検証により、リークがないか

- **G4：代理変数の可能性**  
  ラグ・滞留・比率・ロールアップ等で改善余地があるか

- **G5：現場依存度＋不可逆性（最優先）**  
  見逃しが、安全・品質・法規・資産に対して  
  **不可逆な影響を与えないか**

- **G6：ROI（概算）**  
  便益と運用コストのバランスが取れているか

> **G5 は他のすべてに優先します。**  
> 見逃しが不可逆な場合、この案件は採用されません。

---

## 判定結果（Outcomes）

Gate の結果は、次のいずれかになります。

- **進める**  
  → シャドー運用や A/B テストへ

- **限定パイロット**  
  → 限定範囲での検証とデータ改善

- **やめる**  
  → 予測可能性または価値が不足

- **別案件化**  
  → 非構造データ／人判断前提の専用ルートへ
  → Spec: `specs/predictability-gate_handover_non_structured.yaml`

---

## 責任の所在（Responsibility）

Predictability Gate は、  
**自動承認の仕組みではありません。**

最終的な判断は、  
**運用責任者が Gate 結果を確認した上で行います。**

これは次の立場表明です。

> **AI は責任を取らない。  
> 責任を取るのは人間である。**

---

## 他の概念との関係

このリポジトリは、  
境界と責任を扱う一連の思想とつながっています。

- **idk-lamp**  
  AI が「分からない」と止まるための象徴

- **VCDesign / BOA / RP**  
  責任と境界を設計するための思想群

`predictability-gate` は、  
それらを **運用の現場で実体化した場所**です。

判断停止の実装は BOA 側（RCA / RP）で行われ、  
本リポジトリは **モデル構築前の運用ゲート**を定義します。

---

## 利用について（Usage philosophy）

このリポジトリは自由に参照できます。

- 読んでもよい  
- 参考にしてもよい  
- 使わなくてもよい  

ただし、

> **適用した結果についての責任は、  
> 利用者自身が引き受けるものとします。**

保証はありません。  
サポート義務もありません。

---

## なぜ公開するのか（Why public）

なぜなら、

> **AI を作る方法は無数にあるが、  
> AI を「使わない」と判断する方法は  
> ほとんど共有されていない**

からです。

これは、その空白を  
**明示的な構造として残す試み**です。

---

## ステータス（Status）

このリポジトリは、意図的に最小構成です。

更新されるかもしれません。  
更新されないかもしれません。

どちらも問題ありません。

---

### 著者メモ

これは正解集ではありません。  
**AI を止めるという判断を、  
現場で壊れない形に固定した記録**です。

合わなければ使わなくて構いません。  
それでも「ここに境界がある」ことには意味があります。

---

## Context

このプロジェクトは、AIが判断を止め、
人間の責任へと委ねるべき境界を可視化するための
実践的なシグナルとして設計されています。

- idk-lamp（公式サイト）  
  https://idk-lamp.org/ja

この取り組みは、AI支援システムにおける
設計・責任・境界のあり方を探究する
設計思想 VCDesign から生まれました。

- VCDesign  
  https://vcdesign.org/ja

本リポジトリは単体でも利用・理解できます。  
事前知識は必要ありません。
