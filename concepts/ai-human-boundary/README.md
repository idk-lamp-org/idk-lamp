# ai-human-boundary

ðŸŒ¿ **A Boundary Guide for AI and Humans to Choose Value Together**

This repository is based on the premise not of:
**AI â†’ Human**
but of:
**AI + Human â†’ Value**

> When humans interact with AI,
> things they should purely "know about boundaries"

It is intended to organize and record these minimum requirements.

---

## What is this repository?

This repository does NOT cover:

- How to use AI smartly
- Design for delegating to AI
- Discussions on AI accuracy or performance

It covers only one thing:

> **When thinking together with AI,
> from what point onwards should humans take over?**

---

## Premise: This is not "Design" but a "Guide"

The content written here is:

- Not rules
- Not implementation specifications
- Not meant to enforce judgment criteria

This is **a guide for understanding so that humans do not break in the AI era**.

---

## 1. AI cannot "share uncertainty"

Between humans, these things happen naturally:

- Worrying together
- Betting together
- Holding anxiety together
- Shouldering responsibility together

AI **structurally cannot** do these things.

AI can "handle" uncertainty, but it cannot **share the burden of uncertainty**.

Therefore,

> Backing uncertain choices is the role of humans.

---

## 2. AI's answers are polished but omit uncertainty

AI's answers are:

- Smooth
- Unwavering
- Plausible

However, behind them, there is **no inclusion** of:

- Hesitation
- Anxiety
- Betting
- Risk
- Responsibility

When necessary, you need to recall the following fact:

> **AI omits uncertainty**

This is not a defect, but a property.

---

## 3. AI cannot decide for itself "whether to close the decision"

AI cannot observe whether its output is:

- Used as a reference
- Becoming the basis for action
- substituting someone's judgment

Therefore, it **must have as a structure** conditions such as:

- Cannot close if responsibility is dispersed
- Cannot close if it cannot be undone
- Cannot close if a social context arises

This way of thinking is organized as **Decision Closure**.  
In BOA, it is realized in implementation as RCA / RP.

---

## 4. Humans also need a "Not Known Lamp"

There is a concept called idk-lamp for AI.

However, in reality, **humans need it too**.

- Cannot judge right now
- Cannot decide right now
- Uncertainty is too large right now

This is a signal to convey this to oneself and others.

> Humans being capable of saying "I don't know" is.
> an important skill in the AI era.

---

## 5. AI proposals do not shoulder "responsibility for action"

In the layer of action, there are only the following 3 axes:

- Known / Unknown
- Acted on proposal as is / Human made a judgment
- Human themselves gave up

In other words,

> **Responsibility for action always lies with humans**

AI cannot be an accomplice in action.

---

## 6. AI does not know "human values"

AI possesses no value judgment.

- What to cherish
- What not to lose
- What future to choose
- What risks to tolerate

These are **territories unique to humans**.

AI can infer values, but it **cannot share them**.

---

## 7. AI is an aid, not a substitute

AI can:

- Provide materials for judgment
- Increase perspectives
- Organize thoughts

However, it does:

- Not push your back
- Not participate in the gamble
- Not hold responsibility

That is precisely why,

> **The final judgment should be closed by humans**

Because that is not responsibility, but **the act of choosing value**.

---

## Summary (Short version)

To get along with AI,
these are the only minimum things humans should know:

- AI cannot share uncertainty
- AI's answers omit uncertainty
- AI cannot decide for itself whether to close a decision
- Humans also need a "Not Known Lamp"
- Responsibility for action always lies with humans
- Value judgment is the human domain
- AI is an aid, not a substitute

---

## Positioning of this repository

- Decision Closure:
  Structure that decides whether AI can close a judgment

- ai-human-boundary:
  Guide for humans to choose value together with AI

This repository deals with **how humans stand outside the structure**.

---

## Status

- State: Draft / Living Document
- Purpose: Organization so as not to forget
- Implementation/UI: Out of scope

---

## Finally

As AI becomes smarter,
the "reasons for humans not to decide" will increase.

Even so,

> **What to choose is decided by humans**

This repository exists so as not to forget this boundary.

---

## Context

This project is designed as a practical signal that marks
the boundary where AI systems must stop deciding
and defer responsibility to humans.

- idk-lamp (official site)  
  https://idk-lamp.org/

This work originates from ongoing exploration of
design, responsibility, and boundaries
in AI-assisted systems.

- VCDesign  
  https://vcdesign.org/

This repository can be used and understood independently.  
No prior knowledge is required.
